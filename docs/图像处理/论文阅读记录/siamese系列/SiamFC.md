## 论文信息

论文名：Fully-Convolutional Siamese Networks for Object Tracking（用于目标跟踪的全卷积Siamese网络）

发表作者：Luca Bertinetto, Jack Valmadre, Joao F. Henriques, Andrea Vedaldi, and Philip H.S. Torr

发表单位：Department of Engineering Science, University of Oxford, Oxford, UK（英国牛津大学工程科学系）

笔记：这是一篇研究全卷积网络来进行目标跟踪的论文，为什么要使用卷积来做图像处理呢？全卷积的优势在哪里？

论文发表年份：2016

笔记参考：

[SiamFC论文解读及代码实现](https://blog.csdn.net/weixin_43913124/article/details/123403727)
## Abstract（摘要）

传统上，任意目标跟踪的问题是通过专门在线学习目标外观的模型来解决的，使用视频本身作为唯一的训练数据。尽管这些方法取得了成功，但他们的纯在线方法固有地限制了他们可以学习的模型的丰富性。最近，有人试图利用深层卷积网络的表达能力。然而，当要跟踪的目标事先未知时，需要在线执行随机梯度下降以适应网络的权重，这严重影响了系统的速度。在本文中，我们在ILSVRC15数据集上为视频中的目标检测配备了一个基本的跟踪算法和一个新的端到端训练的孪生神经网络。我们的跟踪器以超过实时的帧速率运行，尽管极其简单，但在多个基准测试中实现了最先进的性能。

## 1 Introduction（引言）

在单目标跟踪任意对象的问题上，其中对象仅仅由第一帧给框出来，通过算法完成后续的跟踪。由于该算法可能被要求跟踪任何任意对象，因此不可能已经收集了数据并训练了特定的检测器。

这篇论文以前，大多是从视频本身提取特征在线学习对象外观的模型，像TLD，KCF，Struck，只能学习相对简单的模型，这个时候没有大量的数据端到端训练是使用深度学习的一道阻力。

最近的几项工作旨在通过使用预训练的深度卷积网络来克服这一限制，该网络是为一项不同但相关的任务学习的。这些方法要么应用“浅层”方法（如相关滤波器），将网络的内部表示作为特征，要么执行SGD（随机梯度下降）来微调网络的多层。各有利弊，使用浅层方法无法充分利用端到端学习，应用SGD以获得最先进结果的方法无法实时运行。

我们提倡另一种方法，即在初始离线阶段训练深度卷积网络来解决更一般的相似性学习问题，然后在跟踪过程中在线简单地评估该函数。本文的主要贡献在于证明，这种方法在现代跟踪基准测试中，以远远超过帧速率要求的速度实现了非常有竞争力的性能。具体来说，我们训练孪生神经网络在更大的搜索图像中定位样本图像。另一个贡献是一种新的连体结构，它与搜索图像完全卷积：通过计算其两个输入的互相关的双线性层实现密集而高效的滑动窗口评估。

我们认为，相似性学习方法已相对被忽视，因为追踪社区无法访问大量标记的数据集。事实上，直到最近，可用的数据集只包含几百个带注释的视频。然而，我们认为，用于视频中目标检测的ILSVRC数据集（此后称为ImageNet视频）的出现，使训练这样一个模型成为可能。此外，train和test使用同域视频进行跟踪的深度模型的公平性是一个争议点，VOT委员会最近禁止了这种做法。我们的模型从ImageNet视频域推广到ALOV/OTB/VOT域，使得跟踪基准的视频可以用于测试。

## 2 Deep Similarity Learning for Tracking（用于跟踪的深度相似性学习）

使用相似性学习可以解决学习跟踪任意对象的问题。我们建议学习一个函数f ( z , x )，该函数将样本图像z与相同大小的候选图像x进行比较，如果两个图像描述相同的对象，则返回高分，否则返回低分。为了在新图像中找到物体的位置，我们可以彻底测试所有可能的位置，并选择与物体过去外观最相似的候选位置。在实验中，我们将简单地使用对象的初始外观作为示例。函数f将从带有标记对象轨迹的视频数据集中学习。


![](image/Pasted%20image%2020240423110952.png)


### 2.1 Fully-Convolutional Siamese Architecture（全卷积孪生结构）

采用卷积嵌入函数φ，采用互相关层结合输出特征图，我们采用了一种判别性方法，对网络进行正负配对训练，并采用logistic loss作为损失函数。

全卷积网络的优点是，我们可以提供一个更大的搜索图像作为网络的输入，而不是相同大小的候选图像，它将在一个单一的评估中计算密集网格上所有转换子窗口的相似性。为了实现这一点，我们使用卷积嵌入函数，并使用互相关层将生成的特征图组合起来f(z,x)=γφ(z)∗φ(x)+b1